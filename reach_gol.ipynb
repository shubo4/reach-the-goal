{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "push_box_game.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0UJIohWGPnpb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#environment creation"
      ],
      "metadata": {
        "id": "cVCeCwWipO9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def randPair(s,e):\n",
        "    return np.random.randint(s,e), np.random.randint(s,e)\n",
        "\n",
        "#finds an array in the \"depth\" dimension of the grid\n",
        "def findLoc(state, obj):\n",
        "    for i in range(0,4):\n",
        "        for j in range(0,4):\n",
        "            if (state[i,j] == obj).all():\n",
        "                return i,j\n",
        "\n",
        "#Initialize stationary grid, all items are placed deterministically\n",
        "def initGrid():\n",
        "    state = np.zeros((4,4,4))\n",
        "    #place player\n",
        "    state[0,1] = np.array([0,0,0,1])\n",
        "    #place wall\n",
        "    state[2,2] = np.array([0,0,1,0])\n",
        "    #place pit\n",
        "    state[1,1] = np.array([0,1,0,0])\n",
        "    #place goal\n",
        "    state[3,3] = np.array([1,0,0,0])\n",
        "    return state\n",
        "\n",
        "#Initialize player in random location, but keep wall, goal and pit stationary\n",
        "def initGridPlayer():\n",
        "    state = np.zeros((4,4,4))\n",
        "    #place player\n",
        "    state[randPair(0,4)] = np.array([0,0,0,1])\n",
        "    #place wall\n",
        "    state[2,2] = np.array([0,0,1,0])\n",
        "    #place pit\n",
        "    state[1,1] = np.array([0,1,0,0])\n",
        "    #place goal\n",
        "    state[1,2] = np.array([1,0,0,0])\n",
        "    \n",
        "    a = findLoc(state, np.array([0,0,0,1])) #find grid position of player (agent)\n",
        "    w = findLoc(state, np.array([0,0,1,0])) #find wall\n",
        "    g = findLoc(state, np.array([1,0,0,0])) #find goal\n",
        "    p = findLoc(state, np.array([0,1,0,0])) #find pit\n",
        "    if (not a or not w or not g or not p):\n",
        "        #print('Invalid grid. Rebuilding..')\n",
        "        return initGridPlayer()\n",
        "    \n",
        "    return state\n",
        "\n",
        "#Initialize grid so that goal, pit, wall, player are all randomly placed\n",
        "def initGridRand():\n",
        "    state = np.zeros((4,4,4))\n",
        "    #place player\n",
        "    state[randPair(0,4)] = np.array([0,0,0,1])\n",
        "    #place wall\n",
        "    state[randPair(0,4)] = np.array([0,0,1,0])\n",
        "    #place pit\n",
        "    state[randPair(0,4)] = np.array([0,1,0,0])\n",
        "    #place goal\n",
        "    state[randPair(0,4)] = np.array([1,0,0,0])\n",
        "    \n",
        "    a = findLoc(state, np.array([0,0,0,1]))\n",
        "    w = findLoc(state, np.array([0,0,1,0]))\n",
        "    g = findLoc(state, np.array([1,0,0,0]))\n",
        "    p = findLoc(state, np.array([0,1,0,0]))\n",
        "    #If any of the \"objects\" are superimposed, just call the function again to re-place\n",
        "    if (not a or not w or not g or not p):\n",
        "        #print('Invalid grid. Rebuilding..')\n",
        "        return initGridRand()\n",
        "    \n",
        "    return state\n",
        "\n",
        "\n",
        "def makeMove(state, action):\n",
        "    #need to locate player in grid\n",
        "    #need to determine what object (if any) is in the new grid spot the player is moving to\n",
        "    player_loc = findLoc(state, np.array([0,0,0,1]))\n",
        "    wall = findLoc(state, np.array([0,0,1,0]))\n",
        "    goal = findLoc(state, np.array([1,0,0,0]))\n",
        "    pit = findLoc(state, np.array([0,1,0,0]))\n",
        "    state = np.zeros((4,4,4))\n",
        "\n",
        "    actions = [[-1,0],[1,0],[0,-1],[0,1]]\n",
        "    #e.g. up => (player row - 1, player column + 0)\n",
        "    new_loc = (player_loc[0] + actions[action][0], player_loc[1] + actions[action][1])\n",
        "    if (new_loc != wall):\n",
        "        if ((np.array(new_loc) <= (3,3)).all() and (np.array(new_loc) >= (0,0)).all()):\n",
        "            state[new_loc][3] = 1\n",
        "\n",
        "    new_player_loc = findLoc(state, np.array([0,0,0,1]))\n",
        "    if (not new_player_loc):\n",
        "        state[player_loc] = np.array([0,0,0,1])\n",
        "    #re-place pit\n",
        "    state[pit][1] = 1\n",
        "    #re-place wall\n",
        "    state[wall][2] = 1\n",
        "    #re-place goal\n",
        "    state[goal][0] = 1\n",
        "\n",
        "    return state\n",
        "\n",
        "def getLoc(state, level):\n",
        "    for i in range(0,4):\n",
        "        for j in range(0,4):\n",
        "            if (state[i,j][level] == 1):\n",
        "                return i,j\n",
        "\n",
        "def getReward(state):\n",
        "    player_loc = getLoc(state, 3)\n",
        "    pit = getLoc(state, 1)\n",
        "    goal = getLoc(state, 0)\n",
        "    if (player_loc == pit):\n",
        "        return -10\n",
        "    elif (player_loc == goal):\n",
        "        return 10\n",
        "    else:\n",
        "        return -1\n",
        "    \n",
        "def dispGrid(state):\n",
        "    grid = np.zeros((4,4), dtype= str)\n",
        "    player_loc = findLoc(state, np.array([0,0,0,1]))\n",
        "    wall = findLoc(state, np.array([0,0,1,0]))\n",
        "    goal = findLoc(state, np.array([1,0,0,0]))\n",
        "    pit = findLoc(state, np.array([0,1,0,0]))\n",
        "    for i in range(0,4):\n",
        "        for j in range(0,4):\n",
        "            grid[i,j] = ' '\n",
        "            \n",
        "    if player_loc:\n",
        "        grid[player_loc] = 'P' #player\n",
        "    if wall:\n",
        "        grid[wall] = 'W' #wall\n",
        "    if goal:\n",
        "        grid[goal] = '+' #goal\n",
        "    if pit:\n",
        "        grid[pit] = '-' #pit\n",
        "    \n",
        "    return grid"
      ],
      "metadata": {
        "id": "HfikY_HqPwPU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Definition of the DQN Model and Replay Buffer"
      ],
      "metadata": {
        "id": "_mxl4DZhpS59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "import random\n",
        "\n",
        "class hidden_unit(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation):\n",
        "        super(hidden_unit, self).__init__()\n",
        "        self.activation = activation\n",
        "        self.nn = nn.Linear(in_channels, out_channels)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.nn(x)\n",
        "        out = self.activation(out)   \n",
        "        return out\n",
        "        \n",
        "class Q_learning(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_layers, out_channels, unit = hidden_unit, activation = F.relu):\n",
        "        super(Q_learning, self).__init__()\n",
        "        assert type(hidden_layers) is list\n",
        "        self.hidden_units = nn.ModuleList()\n",
        "        self.in_channels = in_channels\n",
        "        prev_layer = in_channels\n",
        "        for hidden in hidden_layers:\n",
        "            self.hidden_units.append(unit(prev_layer, hidden, activation))\n",
        "            prev_layer = hidden\n",
        "        self.final_unit = nn.Linear(prev_layer, out_channels)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = x.view(-1,self.in_channels).float()\n",
        "        for unit in self.hidden_units:\n",
        "            out = unit(out)\n",
        "        out = self.final_unit(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'new_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)        \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "hFp96c_3P1iS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#hyper--parameters and training code"
      ],
      "metadata": {
        "id": "Dbffn0nvYHTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "## Include the replay experience\n",
        "\n",
        "epochs = 2000\n",
        "gamma = 0.9 #since it may take several moves to goal, making gamma high\n",
        "epsilon = 1\n",
        "model = Q_learning(64, [150,150], 4, hidden_unit)\n",
        "optimizer = optim.RMSprop(model.parameters(), lr = 1e-2)\n",
        "# optimizer = optim.SGD(model.parameters(), lr = 0.1, momentum = 0)\n",
        "criterion = torch.nn.MSELoss()\n",
        "buffer = 80\n",
        "BATCH_SIZE = 40\n",
        "memory = ReplayMemory(buffer)   \n",
        "\n",
        "for i in tqdm(range(epochs)):\n",
        "    state = initGridRand()\n",
        "    status = 1\n",
        "    step = 0\n",
        "    #while game still in progress\n",
        "    while(status == 1):   \n",
        "        v_state = Variable(torch.from_numpy(state)).view(1,-1)\n",
        "        qval = model(v_state)\n",
        "        if (np.random.random() < epsilon): #choose random action\n",
        "            action = np.random.randint(0,4)\n",
        "        else: #choose best action from Q(s,a) values\n",
        "            action = np.argmax(qval.data)\n",
        "        #Take action, observe new state S'\n",
        "        new_state = makeMove(state, action)\n",
        "        step +=1\n",
        "        v_new_state = Variable(torch.from_numpy(new_state)).view(1,-1)\n",
        "        #Observe reward\n",
        "        reward = getReward(new_state)\n",
        "        memory.push(v_state.data, action, v_new_state.data, reward)\n",
        "        if (len(memory) < buffer): #if buffer not filled, add to it\n",
        "            state = new_state\n",
        "            if reward != -1: #if reached terminal state, update game status\n",
        "                break\n",
        "            else:\n",
        "                continue\n",
        "        transitions = memory.sample(BATCH_SIZE)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        state_batch = Variable(torch.cat(batch.state))\n",
        "        action_batch = Variable(torch.LongTensor(batch.action)).view(-1,1)\n",
        "        new_state_batch = Variable(torch.cat(batch.new_state))\n",
        "        reward_batch = Variable(torch.FloatTensor(batch.reward))\n",
        "        non_final_mask = (reward_batch == -1)\n",
        "        #Let's run our Q function on S to get Q values for all possible actions\n",
        "        qval_batch = model(state_batch)\n",
        "        # we only grad descent on the qval[action], leaving qval[not action] unchanged\n",
        "        state_action_values = qval_batch.gather(1, action_batch)\n",
        "        #Get max_Q(S',a)\n",
        "        with torch.no_grad():\n",
        "            newQ = model(new_state_batch)\n",
        "        maxQ = newQ.max(1)[0]\n",
        "#         if reward == -1: #non-terminal state\n",
        "#             update = (reward + (gamma * maxQ))\n",
        "#         else: #terminal state\n",
        "#             update = reward + 0*maxQ\n",
        "#         y = reward_batch + (reward_batch == -1).float() * gamma *maxQ\n",
        "        y = reward_batch\n",
        "        y[non_final_mask] += gamma * maxQ[non_final_mask]\n",
        "        y = y.view(-1,1)\n",
        "        # print(\"Game #: %s\" % (i,), end='\\r')\n",
        "        loss = criterion(state_action_values, y)\n",
        "        # Optimize the model\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        for p in model.parameters():\n",
        "            p.grad.data.clamp_(-1, 1)\n",
        "        optimizer.step()\n",
        "        state = new_state\n",
        "        if reward != -1:\n",
        "            status = 0\n",
        "        if step >20:\n",
        "            break\n",
        "    if epsilon > 0.1:\n",
        "        epsilon -= (1/epochs)\n",
        "\n",
        "print(\"Training complete!\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZEyik4uP2rs",
        "outputId": "2fa61e79-94c4-4084-9ef9-9884a3753f4f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [01:46<00:00, 18.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#inferencing model"
      ],
      "metadata": {
        "id": "5wyNtkL9P-BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " ## Here is the test of AI\n",
        "def testAlgo(init=0):\n",
        "    i = 0\n",
        "    if init==0:\n",
        "        state = initGrid()\n",
        "    elif init==1:\n",
        "        state = initGridPlayer()\n",
        "    elif init==2:\n",
        "        state = initGridRand()\n",
        "\n",
        "    print(\"Initial State:\")\n",
        "    print(dispGrid(state))\n",
        "    status = 1\n",
        "    #while game still in progress\n",
        "    while(status == 1):\n",
        "        v_state = Variable(torch.from_numpy(state))\n",
        "        qval = model(v_state.view(64))\n",
        "        print(qval)\n",
        "        action = np.argmax(qval.data) #take action with highest Q-value\n",
        "        print('Move #: %s; Taking action: %s' % (i, action))\n",
        "        state = makeMove(state, action)\n",
        "        print(dispGrid(state))\n",
        "        reward = getReward(state)\n",
        "        if reward != -1:\n",
        "            status = 0\n",
        "            print(\"Reward: %s\" % (reward,))\n",
        "        i += 1 #If we're taking more than 10 actions, just stop, we probably can't win this game\n",
        "        if (i > 10):\n",
        "            print(\"Game lost; too many moves.\")\n",
        "            break\n",
        "\n",
        "\n",
        "testAlgo(init=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMG_Grb0P5b1",
        "outputId": "d669ee48-1053-4e58-ed97-0c52a7be2eb2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial State:\n",
            "[[' ' ' ' 'P' ' ']\n",
            " [' ' ' ' ' ' 'W']\n",
            " [' ' ' ' '-' ' ']\n",
            " [' ' '+' ' ' ' ']]\n",
            "tensor([[2.1909, 3.4337, 3.6535, 5.2972]], grad_fn=<AddmmBackward0>)\n",
            "Move #: 0; Taking action: tensor(3)\n",
            "[[' ' ' ' ' ' 'P']\n",
            " [' ' ' ' ' ' 'W']\n",
            " [' ' ' ' '-' ' ']\n",
            " [' ' '+' ' ' ' ']]\n",
            "tensor([[-0.3375,  1.0575,  1.4274,  2.7001]], grad_fn=<AddmmBackward0>)\n",
            "Move #: 1; Taking action: tensor(3)\n",
            "[[' ' ' ' ' ' 'P']\n",
            " [' ' ' ' ' ' 'W']\n",
            " [' ' ' ' '-' ' ']\n",
            " [' ' '+' ' ' ' ']]\n",
            "tensor([[-0.3375,  1.0575,  1.4274,  2.7001]], grad_fn=<AddmmBackward0>)\n",
            "Move #: 2; Taking action: tensor(3)\n",
            "[[' ' ' ' ' ' 'P']\n",
            " [' ' ' ' ' ' 'W']\n",
            " [' ' ' ' '-' ' ']\n",
            " [' ' '+' ' ' ' ']]\n",
            "tensor([[-0.3375,  1.0575,  1.4274,  2.7001]], grad_fn=<AddmmBackward0>)\n",
            "Move #: 3; Taking action: tensor(3)\n",
            "[[' ' ' ' ' ' 'P']\n",
            " [' ' ' ' ' ' 'W']\n",
            " [' ' ' ' '-' ' ']\n",
            " [' ' '+' ' ' ' ']]\n",
            "tensor([[-0.3375,  1.0575,  1.4274,  2.7001]], grad_fn=<AddmmBackward0>)\n",
            "Move #: 4; Taking action: tensor(3)\n",
            "[[' ' ' ' ' ' 'P']\n",
            " [' ' ' ' ' ' 'W']\n",
            " [' ' ' ' '-' ' ']\n",
            " [' ' '+' ' ' ' ']]\n",
            "tensor([[-0.3375,  1.0575,  1.4274,  2.7001]], grad_fn=<AddmmBackward0>)\n",
            "Move #: 5; Taking action: tensor(3)\n",
            "[[' ' ' ' ' ' 'P']\n",
            " [' ' ' ' ' ' 'W']\n",
            " [' ' ' ' '-' ' ']\n",
            " [' ' '+' ' ' ' ']]\n",
            "tensor([[-0.3375,  1.0575,  1.4274,  2.7001]], grad_fn=<AddmmBackward0>)\n",
            "Move #: 6; Taking action: tensor(3)\n",
            "[[' ' ' ' ' ' 'P']\n",
            " [' ' ' ' ' ' 'W']\n",
            " [' ' ' ' '-' ' ']\n",
            " [' ' '+' ' ' ' ']]\n",
            "tensor([[-0.3375,  1.0575,  1.4274,  2.7001]], grad_fn=<AddmmBackward0>)\n",
            "Move #: 7; Taking action: tensor(3)\n",
            "[[' ' ' ' ' ' 'P']\n",
            " [' ' ' ' ' ' 'W']\n",
            " [' ' ' ' '-' ' ']\n",
            " [' ' '+' ' ' ' ']]\n",
            "tensor([[-0.3375,  1.0575,  1.4274,  2.7001]], grad_fn=<AddmmBackward0>)\n",
            "Move #: 8; Taking action: tensor(3)\n",
            "[[' ' ' ' ' ' 'P']\n",
            " [' ' ' ' ' ' 'W']\n",
            " [' ' ' ' '-' ' ']\n",
            " [' ' '+' ' ' ' ']]\n",
            "tensor([[-0.3375,  1.0575,  1.4274,  2.7001]], grad_fn=<AddmmBackward0>)\n",
            "Move #: 9; Taking action: tensor(3)\n",
            "[[' ' ' ' ' ' 'P']\n",
            " [' ' ' ' ' ' 'W']\n",
            " [' ' ' ' '-' ' ']\n",
            " [' ' '+' ' ' ' ']]\n",
            "tensor([[-0.3375,  1.0575,  1.4274,  2.7001]], grad_fn=<AddmmBackward0>)\n",
            "Move #: 10; Taking action: tensor(3)\n",
            "[[' ' ' ' ' ' 'P']\n",
            " [' ' ' ' ' ' 'W']\n",
            " [' ' ' ' '-' ' ']\n",
            " [' ' '+' ' ' ' ']]\n",
            "Game lost; too many moves.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in tqdm(range(total_episodes)):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0, 1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qtable[state,:])\n",
        "            #print(exp_exp_tradeoff, \"action\", action)\n",
        "\n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "            #print(\"action random\", action)\n",
        "            \n",
        "        \n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done (if we're dead) : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "        \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
        "    rewards.append(total_rewards)\n",
        "    \n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "print(qtable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJXgse3XQCO3",
        "outputId": "f6659dad-1218-48fd-bfec-282d57e1ecad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30000/30000 [00:46<00:00, 644.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score over time: 0.5020333333333333\n",
            "[[3.35995692e-01 6.85312929e-02 4.84371168e-02 8.76328308e-02]\n",
            " [1.93292488e-02 6.94751993e-03 6.56083040e-03 2.86630774e-01]\n",
            " [1.33686145e-02 3.44560181e-02 2.32055054e-02 1.57297142e-01]\n",
            " [1.51907729e-02 2.19639425e-03 5.50681239e-03 4.17108707e-02]\n",
            " [4.40091677e-01 3.17389799e-01 3.56037614e-03 4.15271997e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.10786427e-04 2.59460865e-05 1.22351438e-01 5.98695964e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.41552272e-01 6.83545799e-02 2.73747781e-02 4.53743148e-01]\n",
            " [7.10780506e-02 6.72202856e-01 3.29372379e-02 9.41063084e-03]\n",
            " [2.72303233e-01 1.61687104e-03 1.29566574e-02 6.79906388e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.44080036e-01 9.79322685e-02 9.29958961e-01 1.12178573e-01]\n",
            " [3.56200789e-01 9.99844266e-01 1.49496373e-01 2.02158297e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION_LOOKUP = {0: 'LEFT', 1: 'DOWN', 2: 'RIGHT', 3: 'UP'}"
      ],
      "metadata": {
        "id": "5DbhJNfLQGfq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_optimal = [ACTION_LOOKUP[np.argmax(q)] for q in qtable]\n",
        "q_optimal = np.reshape(q_optimal, (4,4))\n",
        "print(q_optimal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OvUKtIbQNEq",
        "outputId": "eb28e374-74a2-4823-d418-9f20c5ec2933"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['LEFT' 'UP' 'UP' 'UP']\n",
            " ['LEFT' 'LEFT' 'RIGHT' 'LEFT']\n",
            " ['UP' 'DOWN' 'LEFT' 'LEFT']\n",
            " ['LEFT' 'RIGHT' 'DOWN' 'LEFT']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#use q table to play frozen lake"
      ],
      "metadata": {
        "id": "fUS-2CF0QWEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "\n",
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", episode)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        \n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        if done:\n",
        "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
        "            env.render()\n",
        "            if new_state == 15:\n",
        "                print(\"We reached our Goal ðŸ†\")\n",
        "            else:\n",
        "                print(\"We fell into a hole â˜ ï¸\")\n",
        "            \n",
        "            # We print the number of step it took.\n",
        "            print(\"Number of steps\", step)\n",
        "            \n",
        "            break\n",
        "        state = new_state\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcfI2y2RQNfs",
        "outputId": "e643d810-7eb1-4b33-ebf3-f46bc49e798e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "We reached our Goal ðŸ†\n",
            "Number of steps 34\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "We reached our Goal ðŸ†\n",
            "Number of steps 6\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "We reached our Goal ðŸ†\n",
            "Number of steps 76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_optimal = [ACTION_LOOKUP[np.argmax(q)] for q in qtable]\n",
        "q_optimal = np.reshape(q_optimal, (4,4))\n",
        "print(q_optimal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdGD6cRRQcDt",
        "outputId": "33e9ca53-6ecd-42a7-c5f7-56cbeb0f7e5f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['LEFT' 'UP' 'UP' 'UP']\n",
            " ['LEFT' 'LEFT' 'RIGHT' 'LEFT']\n",
            " ['UP' 'DOWN' 'LEFT' 'LEFT']\n",
            " ['LEFT' 'RIGHT' 'DOWN' 'LEFT']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aTGeaTYxQg8M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}